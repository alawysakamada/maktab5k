{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":962336,"sourceType":"datasetVersion","datasetId":524336},{"sourceId":9602905,"sourceType":"datasetVersion","datasetId":5858702},{"sourceId":9605807,"sourceType":"datasetVersion","datasetId":5860745},{"sourceId":9607029,"sourceType":"datasetVersion","datasetId":5861645},{"sourceId":9618132,"sourceType":"datasetVersion","datasetId":5869874},{"sourceId":9499040,"sourceType":"datasetVersion","datasetId":5780646},{"sourceId":9619170,"sourceType":"datasetVersion","datasetId":5870647}],"dockerImageVersionId":30788,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\n# Muat dataset dari file CSV\ndf = pd.read_csv('/kaggle/input/arabic-library/my_csv.csv')\n\n# Muat daftar judul buku dari file teks\nwith open('/kaggle/input/bookst2/book_titles1.txt', 'r', encoding='utf-8') as file:\n    book_titles = [line.strip() for line in file.readlines()]\n\n# Inisialisasi daftar untuk menyimpan judul buku yang tidak ditemukan\nnot_found_titles = []\n\n# Inisialisasi DataFrame kosong untuk hasil\nfiltered_df = pd.DataFrame()\n\n# Mencari setiap judul buku\nfor title in book_titles:\n    # Filter untuk mengambil semua baris yang memiliki judul buku\n    matching_rows = df[df['Book_name'].str.contains(title, case=False, na=False)]\n    \n    # Jika ada baris yang cocok, tambahkan ke filtered_df\n    if not matching_rows.empty:\n        filtered_df = pd.concat([filtered_df, matching_rows], ignore_index=True)\n    else:\n        # Jika tidak ditemukan, tambahkan judul ke not_found_titles\n        not_found_titles.append(title)\n\n# Simpan hasil yang ditemukan ke file CSV\noutput_file = '/kaggle/working/books.csv'\nfiltered_df.to_csv(output_file, index=False)\n\n# Simpan judul buku yang tidak ditemukan ke file log\nif not_found_titles:\n    with open('/kaggle/working/not_found.log', 'w', encoding='utf-8') as log_file:\n        for title in not_found_titles:\n            log_file.write(f\"{title}\\n\")\n\nprint(f\"Hasil telah disimpan dalam {output_file}.\")\nif not_found_titles:\n    print(f\"Judul buku yang tidak ditemukan telah dicatat dalam not_found_titles.log.\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Baca file CSV\ndf = pd.read_csv('/kaggle/working/not_found.log')\n\n# Tampilkan tabel secara rapi di notebook\ndf.head()  # Menampilkan 5 baris pertama\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\n# Set CUDA_LAUNCH_BLOCKING untuk mendapatkan lebih banyak informasi tentang kesalahan\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n\n# Import library yang diperlukan\nimport pandas as pd\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\nfrom datasets import Dataset\nimport json\nimport wandb\n\n# Login ke Hugging Face\nfrom huggingface_hub import login\nlogin(token=\"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\")\n\n# Login ke W&B menggunakan API key\nwandb.login(key=\"5a646a4e2997f8ff868dfe4df325accd8a13b4b6\")\n\n\n# Memuat dataset\n# Memuat dataset satu per satu\ndf1 = pd.read_csv('/kaggle/input/maktab5k/AClean_part_1_part_1_5k/AClean_part_1_part_1_part_1.csv')\n#df2 = pd.read_csv('/kaggle/input/maktab5k/AClean_part_1_part_1_5k/AClean_part_1_part_1_part_2.csv')\n#df3 = pd.read_csv('/kaggle/input/maktab25/A_1_25/AClean_part_1_part_3.csv')\n\n# Menggabungkan DataFrame\ndf = pd.concat([df1], ignore_index=True)\n\n\n# Memeriksa label unik\nunique_labels = df['Book_name'].unique()\nprint(\"Label unik sebelum mapping:\", unique_labels)\n\n# Membuat mapping dari label lama ke label baru yang berurutan\nlabel_mapping = {label: idx for idx, label in enumerate(sorted(unique_labels))}\ndf['Book_name'] = df['Book_name'].map(label_mapping)\n\n# Memeriksa label unik setelah mapping\nunique_labels_after_mapping = df['Book_name'].unique()\nprint(\"Label unik setelah mapping:\", unique_labels_after_mapping)\nprint(\"Jumlah kelas:\", len(unique_labels_after_mapping))  # Harus sesuai dengan num_labels\n\n# Pastikan kolom 'text' dalam bentuk string\ndf['text'] = df['text'].astype(str)\n\n# Memuat tokenizer dan model\nnum_labels = len(unique_labels_after_mapping)  # Jumlah kelas\ntokenizer = AutoTokenizer.from_pretrained(\"asafaya/bert-base-arabic\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"asafaya/bert-base-arabic\", \n    num_labels=num_labels\n)\n\n# Pindahkan model ke GPU jika tersedia\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Konversi DataFrame menjadi Dataset Hugging Face\ndataset = Dataset.from_pandas(df[['text', 'Book_name']])\n\n# Fungsi tokenisasi untuk digunakan dengan `map`\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n\n# Tokenisasi dataset\ntokenized_dataset = dataset.map(tokenize_function, batched=True)\n\n# Menentukan kolom yang digunakan sebagai input dan label\ntokenized_dataset = tokenized_dataset.rename_column(\"Book_name\", \"labels\")\n\n# Menetapkan format dataset dan mengatur kolom yang digunakan\ntokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n\n# Membuat data collator dengan padding dinamis\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n# Validasi label\ninvalid_labels = tokenized_dataset.filter(lambda x: x['labels'] < 0 or x['labels'] >= num_labels)\nif len(invalid_labels) > 0:\n    print(f\"Ada {len(invalid_labels)} label yang tidak valid!\")\n\n# Mengatur argumen pelatihan\ntraining_args = TrainingArguments(\n    output_dir='./models/arabic_sentence_transformer',\n    eval_strategy=\"no\",  # Tidak melakukan evaluasi\n    learning_rate=1e-5,\n    per_device_train_batch_size=8,  # Kurangi ukuran batch jika perlu\n    num_train_epochs=5,\n    weight_decay=0.01,\n    save_total_limit=2,\n    save_strategy=\"epoch\",\n    logging_dir='./logs',\n    logging_steps=10,\n)\n\n\n# Membuat Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    data_collator=data_collator,\n)\n\nprint(\"Trainer telah dibuat, mulai melatih model...\")\n\n# Melatih model\ntrainer.train()\n\n# Menyimpan model dan tokenizer\nmodel.save_pretrained('./models/arabic_sentence_transformer')\ntokenizer.save_pretrained('./models/arabic_sentence_transformer')\nprint(\"Model dan tokenizer disimpan.\")\n\n# Menyimpan metrik pelatihan ke file JSON\nmetrics = {\n    \"loss\": trainer.state.log_history[-1][\"loss\"],\n    \"epoch\": trainer.state.log_history[-1][\"epoch\"],\n}\n\nwith open('./models/training_metrics.json', 'w') as f:\n    json.dump(metrics, f)\n\nprint(\"Metrik pelatihan disimpan ke training_metrics.json.\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import library yang diperlukan\nimport pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\nfrom sklearn.metrics import classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Memuat dataset uji\ntest_file = '/kaggle/input/maktab5k/AClean_part_1_part_1_5k/AClean_part_1_part_1_part_5.csv'\ndf = pd.read_csv(test_file)\n\n# Memuat model dan tokenizer\nmodel_dir = './models/arabic_sentence_transformer'  # Ganti dengan model yang sesuai jika diperlukan\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_dir)\n\n# Menentukan perangkat (CPU/GPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Menentukan panjang maksimum untuk tokenisasi\nmax_length = 512  # Disarankan menggunakan panjang yang konsisten dengan model (contohnya: 512)\n\n# Tokenisasi data uji dengan padding dan truncation\ntokens = tokenizer(\n    df['text'].tolist(), \n    padding=True, \n    truncation=True, \n    max_length=max_length, \n    return_tensors=\"pt\"\n)\n\n# Konversi label ke tensor\nlabels = torch.tensor(df['Book_name'].astype(int).tolist())\n\n# Membuat DataLoader\ndataset = TensorDataset(tokens['input_ids'], tokens['attention_mask'], labels)\ndataloader = DataLoader(dataset, batch_size=8)  # Menggunakan batch size lebih kecil untuk menghindari masalah memori\n\n# Melakukan prediksi\npredictions = []\nmodel.eval()\nwith torch.no_grad():\n    for batch in dataloader:\n        # Memindahkan tensor ke perangkat yang sesuai (CPU/GPU)\n        input_ids, attention_mask, _ = [t.to(device) for t in batch]\n        outputs = model(input_ids, attention_mask=attention_mask)\n        batch_predictions = torch.argmax(outputs.logits, dim=1)\n        predictions.extend(batch_predictions.cpu().tolist())  # Memindahkan prediksi ke CPU sebelum menyimpannya\n\n# Menampilkan laporan klasifikasi\nreport = classification_report(labels.tolist(), predictions, output_dict=True)\n\n# Mengonversi laporan ke DataFrame untuk visualisasi\nreport_df = pd.DataFrame(report).transpose()\n\n# Menampilkan laporan klasifikasi\nprint(report_df)\n\n# Visualisasi hasil\nplt.figure(figsize=(10, 5))\nsns.heatmap(report_df.iloc[:-1, :-1], annot=True, fmt=\".2f\", cmap='Blues')\nplt.title('Classification Report Heatmap')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!kaggle kernels output danialmahdi/shamela --path /kaggle/working/models\n\n\n# Pindahkan file ke folder yang bisa diakses dan diunduhmv filtered_books.csv /kaggle/working/filtered_books.csv\n!mv filtered_books.csv /kaggle/working/filtered_books.csv\n!mv not_found_titles.log /kaggle/working/not_found_titles.log\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"GENERATE Q&A","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments\nfrom evaluate import load\nfrom sklearn.model_selection import train_test_split\nimport wandb\n\n# Login ke W&B menggunakan API key\nwandb.login(key=\"5a646a4e2997f8ff868dfe4df325accd8a13b4b6\")\n\n# Inisialisasi Tokenizer dan Model\nmodel_name = \"MIIB-NLP/Arabic-question-generation\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\nclass ArabicQuestionAnswerDataset(Dataset):\n    def __init__(self, data, tokenizer, max_length=512):\n        self.tokenizer = tokenizer\n        self.data = data\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        context = self.data.loc[idx, 'text']\n        answer = context  # Menggunakan context yang sama untuk answer\n\n        # Tokenisasi\n        inputs = self.tokenizer(\n            context,\n            return_tensors=\"pt\",\n            truncation=True,\n            max_length=self.max_length,\n            padding=\"max_length\"\n        )\n        labels = self.tokenizer(\n            answer,\n            return_tensors=\"pt\",\n            truncation=True,\n            max_length=self.max_length,\n            padding=\"max_length\"\n        ).input_ids\n\n        # Mengembalikan sebagai dictionary yang sesuai\n        return {\n            'input_ids': inputs['input_ids'].squeeze(),\n            'attention_mask': inputs['attention_mask'].squeeze(),\n            'labels': labels.squeeze()\n        }\n\n\n# Memuat dataset dari CSV dan bagi menjadi train-test split\ndef load_and_split_data(csv_file):\n    df = pd.read_csv(csv_file)\n    train_data, val_data = train_test_split(df, test_size=0.2)\n    return train_data.reset_index(drop=True), val_data.reset_index(drop=True)\n\n# Menghasilkan pertanyaan menggunakan model yang di fine-tune\ndef generate_questions(model, tokenizer, context, max_length=50):\n    inputs = tokenizer(context, return_tensors=\"pt\", truncation=True, max_length=512).input_ids.to(model.device)\n    outputs = model.generate(inputs, max_length=max_length)\n    question = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return question\n\n# Fine-tuning menggunakan Trainer dari Hugging Face\ndef fine_tune_model(train_dataset, val_dataset, output_dir=\"output_model\", epochs=3, batch_size=4):\n    training_args = Seq2SeqTrainingArguments(\n        output_dir=output_dir,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        predict_with_generate=True,\n        evaluation_strategy=\"steps\",\n        save_steps=500,\n        eval_steps=500,\n        logging_dir='./logs',\n        logging_steps=100,\n        num_train_epochs=epochs,\n        save_total_limit=2,\n        load_best_model_at_end=True,\n    )\n\n    trainer = Seq2SeqTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n    )\n\n    trainer.train()\n\n# Simpan model yang sudah di fine-tune\ndef save_fine_tuned_model(model, output_dir):\n    model.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n\n# Fungsi utama untuk menjalankan fine-tuning\ndef main(csv_file):\n    # Load dan bagi dataset\n    train_data, val_data = load_and_split_data(csv_file)\n\n    # Siapkan dataset untuk fine-tuning\n    train_dataset = ArabicQuestionAnswerDataset(train_data, tokenizer)\n    val_dataset = ArabicQuestionAnswerDataset(val_data, tokenizer)\n\n    # Fine-tune model\n    fine_tune_model(train_dataset, val_dataset)\n\n    # Simpan model yang sudah di fine-tune\n    save_fine_tuned_model(model, \"fine_tuned_arabic_question_model\")\n\n    # Hasilkan pertanyaan dan jawaban serta simpan ke CSV\n    df = pd.read_csv(csv_file)\n    df['Generated_Question'] = df['text'].apply(lambda x: generate_questions(model, tokenizer, x))\n    df['Generated_Answer'] = df['text']  # Answer tetap dari kolom text yang sama\n    \n    # Simpan ke file CSV baru\n    output_csv = \"generated_questions_answers.csv\"\n    df.to_csv(output_csv, index=False)\n    print(f\"Hasil pertanyaan dan jawaban disimpan di: {output_csv}\")\n\nif __name__ == \"__main__\":\n    # Ganti dengan path dataset CSV Anda\n    csv_file = \"/kaggle/working/books.csv\"\n    main(csv_file)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T09:34:41.211274Z","iopub.execute_input":"2024-10-14T09:34:41.211787Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkoleksigame0001\u001b[0m (\u001b[33mkoleksigame0001-universitas-diponegoro\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112503999998403, max=1.0â€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90cf13e31522413ca0bc2aa6edd2998b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241014_093455-phoqzvzi</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/koleksigame0001-universitas-diponegoro/huggingface/runs/phoqzvzi' target=\"_blank\">output_model</a></strong> to <a href='https://wandb.ai/koleksigame0001-universitas-diponegoro/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/koleksigame0001-universitas-diponegoro/huggingface' target=\"_blank\">https://wandb.ai/koleksigame0001-universitas-diponegoro/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/koleksigame0001-universitas-diponegoro/huggingface/runs/phoqzvzi' target=\"_blank\">https://wandb.ai/koleksigame0001-universitas-diponegoro/huggingface/runs/phoqzvzi</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2295' max='2295' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2295/2295 37:05, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>1.104400</td>\n      <td>0.296863</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.305200</td>\n      <td>0.068386</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.184800</td>\n      <td>0.046862</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.147100</td>\n      <td>0.041915</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install evaluate","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}