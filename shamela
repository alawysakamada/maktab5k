{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9602905,"sourceType":"datasetVersion","datasetId":5858702},{"sourceId":9605807,"sourceType":"datasetVersion","datasetId":5860745},{"sourceId":9607029,"sourceType":"datasetVersion","datasetId":5861645}],"dockerImageVersionId":30788,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# Set CUDA_LAUNCH_BLOCKING untuk mendapatkan lebih banyak informasi tentang kesalahan\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n\n# Import library yang diperlukan\nimport pandas as pd\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\nfrom datasets import Dataset\nimport json\nimport wandb\n\n# Login ke Hugging Face\nfrom huggingface_hub import login\nlogin(token=\"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\")\n\n# Login ke W&B menggunakan API key\nwandb.login(key=\"5a646a4e2997f8ff868dfe4df325accd8a13b4b6\")\n\n\n# Memuat dataset\n# Memuat dataset satu per satu\ndf1 = pd.read_csv('/kaggle/input/maktab5k/AClean_part_1_part_1_5k/AClean_part_1_part_1_part_1.csv')\n#df2 = pd.read_csv('/kaggle/input/maktab5k/AClean_part_1_part_1_5k/AClean_part_1_part_1_part_2.csv')\n#df3 = pd.read_csv('/kaggle/input/maktab25/A_1_25/AClean_part_1_part_3.csv')\n\n# Menggabungkan DataFrame\ndf = pd.concat([df1], ignore_index=True)\n\n\n# Memeriksa label unik\nunique_labels = df['Book_name'].unique()\nprint(\"Label unik sebelum mapping:\", unique_labels)\n\n# Membuat mapping dari label lama ke label baru yang berurutan\nlabel_mapping = {label: idx for idx, label in enumerate(sorted(unique_labels))}\ndf['Book_name'] = df['Book_name'].map(label_mapping)\n\n# Memeriksa label unik setelah mapping\nunique_labels_after_mapping = df['Book_name'].unique()\nprint(\"Label unik setelah mapping:\", unique_labels_after_mapping)\nprint(\"Jumlah kelas:\", len(unique_labels_after_mapping))  # Harus sesuai dengan num_labels\n\n# Pastikan kolom 'text' dalam bentuk string\ndf['text'] = df['text'].astype(str)\n\n# Memuat tokenizer dan model\nnum_labels = len(unique_labels_after_mapping)  # Jumlah kelas\ntokenizer = AutoTokenizer.from_pretrained(\"asafaya/bert-base-arabic\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"asafaya/bert-base-arabic\", \n    num_labels=num_labels\n)\n\n# Pindahkan model ke GPU jika tersedia\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Konversi DataFrame menjadi Dataset Hugging Face\ndataset = Dataset.from_pandas(df[['text', 'Book_name']])\n\n# Fungsi tokenisasi untuk digunakan dengan `map`\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n\n# Tokenisasi dataset\ntokenized_dataset = dataset.map(tokenize_function, batched=True)\n\n# Menentukan kolom yang digunakan sebagai input dan label\ntokenized_dataset = tokenized_dataset.rename_column(\"Book_name\", \"labels\")\n\n# Menetapkan format dataset dan mengatur kolom yang digunakan\ntokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n\n# Membuat data collator dengan padding dinamis\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n# Validasi label\ninvalid_labels = tokenized_dataset.filter(lambda x: x['labels'] < 0 or x['labels'] >= num_labels)\nif len(invalid_labels) > 0:\n    print(f\"Ada {len(invalid_labels)} label yang tidak valid!\")\n\n# Mengatur argumen pelatihan\ntraining_args = TrainingArguments(\n    output_dir='./models/arabic_sentence_transformer',\n    eval_strategy=\"no\",  # Tidak melakukan evaluasi\n    learning_rate=1e-5,\n    per_device_train_batch_size=8,  # Kurangi ukuran batch jika perlu\n    num_train_epochs=5,\n    weight_decay=0.01,\n    save_total_limit=2,\n    save_strategy=\"epoch\",\n    logging_dir='./logs',\n    logging_steps=10,\n)\n\n\n# Membuat Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    data_collator=data_collator,\n)\n\nprint(\"Trainer telah dibuat, mulai melatih model...\")\n\n# Melatih model\ntrainer.train()\n\n# Menyimpan model dan tokenizer\nmodel.save_pretrained('./models/arabic_sentence_transformer')\ntokenizer.save_pretrained('./models/arabic_sentence_transformer')\nprint(\"Model dan tokenizer disimpan.\")\n\n# Menyimpan metrik pelatihan ke file JSON\nmetrics = {\n    \"loss\": trainer.state.log_history[-1][\"loss\"],\n    \"epoch\": trainer.state.log_history[-1][\"epoch\"],\n}\n\nwith open('./models/training_metrics.json', 'w') as f:\n    json.dump(metrics, f)\n\nprint(\"Metrik pelatihan disimpan ke training_metrics.json.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-12T08:37:46.467800Z","iopub.execute_input":"2024-10-12T08:37:46.468633Z","iopub.status.idle":"2024-10-12T09:03:39.024773Z","shell.execute_reply.started":"2024-10-12T08:37:46.468589Z","shell.execute_reply":"2024-10-12T09:03:39.020308Z"},"trusted":true},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"Token is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\nLabel unik sebelum mapping: ['1000 سؤال وجواب في القرآن' '48 سؤالا في الصيام'\n '60 سؤال وجواب في أحكام الحيض' 'آثار ابن باديس'\n 'آثار الإمام محمد البشير الإبراهيمي' 'آثار البلاد وأخبار العباد']\nLabel unik setelah mapping: [0 1 2 3 4 5]\nJumlah kelas: 6\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at asafaya/bert-base-arabic and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3625a730b6b4b47aa79d72a015d7ebb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00eeb19c3000480cacee6d190e64e633"}},"metadata":{}},{"name":"stdout","text":"Trainer telah dibuat, mulai melatih model...\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111250704444602, max=1.0)…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ff88de23c9146fb9c4e333e73ddefc0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241012_083753-vmbvh4ts</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/koleksigame0001-universitas-diponegoro/huggingface/runs/vmbvh4ts' target=\"_blank\">./models/arabic_sentence_transformer</a></strong> to <a href='https://wandb.ai/koleksigame0001-universitas-diponegoro/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/koleksigame0001-universitas-diponegoro/huggingface' target=\"_blank\">https://wandb.ai/koleksigame0001-universitas-diponegoro/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/koleksigame0001-universitas-diponegoro/huggingface/runs/vmbvh4ts' target=\"_blank\">https://wandb.ai/koleksigame0001-universitas-diponegoro/huggingface/runs/vmbvh4ts</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3125/3125 25:40, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>1.421600</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.200800</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.913600</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.734900</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.708300</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.543600</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.500500</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.329800</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.423200</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.194000</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.278700</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.136400</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.286600</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.457500</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.230200</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.165300</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.275700</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.248600</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.125100</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.254200</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.191900</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.149300</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.043600</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.119600</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.188800</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.212000</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.119600</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.135500</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.153500</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.065100</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.055300</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.228200</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.222300</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.126400</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.256600</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.084900</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.086700</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.135800</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.339800</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.270200</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.085900</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.124900</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.223700</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.189200</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.109700</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.009500</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.104400</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.261200</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.091800</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.105700</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.041500</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.297800</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.326000</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.132000</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.072400</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.192900</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.060500</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.041300</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.181700</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.186600</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.005800</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.135200</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.107900</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.034800</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.057300</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.115100</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.047900</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.012600</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.009800</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.079100</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>0.233400</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.004300</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.008300</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.056700</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.012900</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.085800</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>0.082900</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.002300</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>0.316900</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.295000</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>0.008700</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.002600</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>0.157200</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.040700</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.014300</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.003500</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>0.086300</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.161800</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>0.094300</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.054500</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>0.045900</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.117200</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>0.112900</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>0.018000</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.009200</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>0.095600</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>0.007100</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>0.026900</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>0.002800</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.041800</td>\n    </tr>\n    <tr>\n      <td>1010</td>\n      <td>0.070500</td>\n    </tr>\n    <tr>\n      <td>1020</td>\n      <td>0.008100</td>\n    </tr>\n    <tr>\n      <td>1030</td>\n      <td>0.021400</td>\n    </tr>\n    <tr>\n      <td>1040</td>\n      <td>0.087800</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.242000</td>\n    </tr>\n    <tr>\n      <td>1060</td>\n      <td>0.004000</td>\n    </tr>\n    <tr>\n      <td>1070</td>\n      <td>0.011400</td>\n    </tr>\n    <tr>\n      <td>1080</td>\n      <td>0.084300</td>\n    </tr>\n    <tr>\n      <td>1090</td>\n      <td>0.106300</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.042700</td>\n    </tr>\n    <tr>\n      <td>1110</td>\n      <td>0.002300</td>\n    </tr>\n    <tr>\n      <td>1120</td>\n      <td>0.021800</td>\n    </tr>\n    <tr>\n      <td>1130</td>\n      <td>0.001300</td>\n    </tr>\n    <tr>\n      <td>1140</td>\n      <td>0.004200</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.095600</td>\n    </tr>\n    <tr>\n      <td>1160</td>\n      <td>0.069300</td>\n    </tr>\n    <tr>\n      <td>1170</td>\n      <td>0.136600</td>\n    </tr>\n    <tr>\n      <td>1180</td>\n      <td>0.006100</td>\n    </tr>\n    <tr>\n      <td>1190</td>\n      <td>0.138600</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.007000</td>\n    </tr>\n    <tr>\n      <td>1210</td>\n      <td>0.076000</td>\n    </tr>\n    <tr>\n      <td>1220</td>\n      <td>0.001700</td>\n    </tr>\n    <tr>\n      <td>1230</td>\n      <td>0.234700</td>\n    </tr>\n    <tr>\n      <td>1240</td>\n      <td>0.001800</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.049600</td>\n    </tr>\n    <tr>\n      <td>1260</td>\n      <td>0.016100</td>\n    </tr>\n    <tr>\n      <td>1270</td>\n      <td>0.004400</td>\n    </tr>\n    <tr>\n      <td>1280</td>\n      <td>0.001300</td>\n    </tr>\n    <tr>\n      <td>1290</td>\n      <td>0.051500</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.060100</td>\n    </tr>\n    <tr>\n      <td>1310</td>\n      <td>0.002500</td>\n    </tr>\n    <tr>\n      <td>1320</td>\n      <td>0.019900</td>\n    </tr>\n    <tr>\n      <td>1330</td>\n      <td>0.009300</td>\n    </tr>\n    <tr>\n      <td>1340</td>\n      <td>0.006000</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.005400</td>\n    </tr>\n    <tr>\n      <td>1360</td>\n      <td>0.069500</td>\n    </tr>\n    <tr>\n      <td>1370</td>\n      <td>0.016800</td>\n    </tr>\n    <tr>\n      <td>1380</td>\n      <td>0.092500</td>\n    </tr>\n    <tr>\n      <td>1390</td>\n      <td>0.020700</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.020700</td>\n    </tr>\n    <tr>\n      <td>1410</td>\n      <td>0.083400</td>\n    </tr>\n    <tr>\n      <td>1420</td>\n      <td>0.002100</td>\n    </tr>\n    <tr>\n      <td>1430</td>\n      <td>0.006600</td>\n    </tr>\n    <tr>\n      <td>1440</td>\n      <td>0.083600</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>0.012500</td>\n    </tr>\n    <tr>\n      <td>1460</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>1470</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>1480</td>\n      <td>0.100100</td>\n    </tr>\n    <tr>\n      <td>1490</td>\n      <td>0.002100</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.070300</td>\n    </tr>\n    <tr>\n      <td>1510</td>\n      <td>0.001700</td>\n    </tr>\n    <tr>\n      <td>1520</td>\n      <td>0.005300</td>\n    </tr>\n    <tr>\n      <td>1530</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>1540</td>\n      <td>0.002200</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>0.101200</td>\n    </tr>\n    <tr>\n      <td>1560</td>\n      <td>0.001600</td>\n    </tr>\n    <tr>\n      <td>1570</td>\n      <td>0.001500</td>\n    </tr>\n    <tr>\n      <td>1580</td>\n      <td>0.093500</td>\n    </tr>\n    <tr>\n      <td>1590</td>\n      <td>0.004100</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.001600</td>\n    </tr>\n    <tr>\n      <td>1610</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>1620</td>\n      <td>0.030800</td>\n    </tr>\n    <tr>\n      <td>1630</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>1640</td>\n      <td>0.001400</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>1660</td>\n      <td>0.001200</td>\n    </tr>\n    <tr>\n      <td>1670</td>\n      <td>0.002300</td>\n    </tr>\n    <tr>\n      <td>1680</td>\n      <td>0.064000</td>\n    </tr>\n    <tr>\n      <td>1690</td>\n      <td>0.008200</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>1710</td>\n      <td>0.080400</td>\n    </tr>\n    <tr>\n      <td>1720</td>\n      <td>0.017600</td>\n    </tr>\n    <tr>\n      <td>1730</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>1740</td>\n      <td>0.078600</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>0.067100</td>\n    </tr>\n    <tr>\n      <td>1760</td>\n      <td>0.001400</td>\n    </tr>\n    <tr>\n      <td>1770</td>\n      <td>0.001500</td>\n    </tr>\n    <tr>\n      <td>1780</td>\n      <td>0.064800</td>\n    </tr>\n    <tr>\n      <td>1790</td>\n      <td>0.034200</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>1810</td>\n      <td>0.084700</td>\n    </tr>\n    <tr>\n      <td>1820</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>1830</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>1840</td>\n      <td>0.001500</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>1860</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>1870</td>\n      <td>0.053600</td>\n    </tr>\n    <tr>\n      <td>1880</td>\n      <td>0.094400</td>\n    </tr>\n    <tr>\n      <td>1890</td>\n      <td>0.005100</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>1910</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>1920</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>1930</td>\n      <td>0.064200</td>\n    </tr>\n    <tr>\n      <td>1940</td>\n      <td>0.001300</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>1960</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>1970</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>1980</td>\n      <td>0.001200</td>\n    </tr>\n    <tr>\n      <td>1990</td>\n      <td>0.070900</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>2010</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>2020</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>2030</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>2040</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>2050</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>2060</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>2070</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>2080</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>2090</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>2110</td>\n      <td>0.001900</td>\n    </tr>\n    <tr>\n      <td>2120</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>2130</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>2140</td>\n      <td>0.131700</td>\n    </tr>\n    <tr>\n      <td>2150</td>\n      <td>0.001200</td>\n    </tr>\n    <tr>\n      <td>2160</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>2170</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>2180</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>2190</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>2210</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>2220</td>\n      <td>0.002500</td>\n    </tr>\n    <tr>\n      <td>2230</td>\n      <td>0.007100</td>\n    </tr>\n    <tr>\n      <td>2240</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>2250</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>2260</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>2270</td>\n      <td>0.001200</td>\n    </tr>\n    <tr>\n      <td>2280</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>2290</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.064000</td>\n    </tr>\n    <tr>\n      <td>2310</td>\n      <td>0.052200</td>\n    </tr>\n    <tr>\n      <td>2320</td>\n      <td>0.001900</td>\n    </tr>\n    <tr>\n      <td>2330</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>2340</td>\n      <td>0.001200</td>\n    </tr>\n    <tr>\n      <td>2350</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>2360</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>2370</td>\n      <td>0.024600</td>\n    </tr>\n    <tr>\n      <td>2380</td>\n      <td>0.051700</td>\n    </tr>\n    <tr>\n      <td>2390</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.001800</td>\n    </tr>\n    <tr>\n      <td>2410</td>\n      <td>0.003300</td>\n    </tr>\n    <tr>\n      <td>2420</td>\n      <td>0.001300</td>\n    </tr>\n    <tr>\n      <td>2430</td>\n      <td>0.105100</td>\n    </tr>\n    <tr>\n      <td>2440</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>2450</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>2460</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>2470</td>\n      <td>0.102700</td>\n    </tr>\n    <tr>\n      <td>2480</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>2490</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>2510</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>2520</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>2530</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>2540</td>\n      <td>0.065500</td>\n    </tr>\n    <tr>\n      <td>2550</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>2560</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>2570</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>2580</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>2590</td>\n      <td>0.006200</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>2610</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>2620</td>\n      <td>0.112000</td>\n    </tr>\n    <tr>\n      <td>2630</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>2640</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>2650</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>2660</td>\n      <td>0.047600</td>\n    </tr>\n    <tr>\n      <td>2670</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>2680</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>2690</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.001200</td>\n    </tr>\n    <tr>\n      <td>2710</td>\n      <td>0.039300</td>\n    </tr>\n    <tr>\n      <td>2720</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>2730</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>2740</td>\n      <td>0.028000</td>\n    </tr>\n    <tr>\n      <td>2750</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>2760</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>2770</td>\n      <td>0.002000</td>\n    </tr>\n    <tr>\n      <td>2780</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>2790</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.026400</td>\n    </tr>\n    <tr>\n      <td>2810</td>\n      <td>0.001700</td>\n    </tr>\n    <tr>\n      <td>2820</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>2830</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>2840</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>2850</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>2860</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>2870</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>2880</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>2890</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>2910</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>2920</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>2930</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>2940</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>2950</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>2960</td>\n      <td>0.008200</td>\n    </tr>\n    <tr>\n      <td>2970</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>2980</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>2990</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.050900</td>\n    </tr>\n    <tr>\n      <td>3010</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>3020</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>3030</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>3040</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>3050</td>\n      <td>0.013700</td>\n    </tr>\n    <tr>\n      <td>3060</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>3070</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>3080</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>3090</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>3110</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>3120</td>\n      <td>0.002000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Model dan tokenizer disimpan.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 119\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel dan tokenizer disimpan.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Menyimpan metrik pelatihan ke file JSON\u001b[39;00m\n\u001b[1;32m    118\u001b[0m metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_history\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m: trainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mlog_history[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    121\u001b[0m }\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./models/training_metrics.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    124\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(metrics, f)\n","\u001b[0;31mKeyError\u001b[0m: 'loss'"],"ename":"KeyError","evalue":"'loss'","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"# Import library yang diperlukan\nimport pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\nfrom sklearn.metrics import classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Memuat dataset uji\ntest_file = '/kaggle/input/maktab5k/AClean_part_1_part_1_5k/AClean_part_1_part_1_part_5.csv'\ndf = pd.read_csv(test_file)\n\n# Memuat model dan tokenizer\nmodel_dir = './models/arabic_sentence_transformer'  # Ganti dengan model yang sesuai jika diperlukan\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_dir)\n\n# Menentukan perangkat (CPU/GPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Menentukan panjang maksimum untuk tokenisasi\nmax_length = 512  # Disarankan menggunakan panjang yang konsisten dengan model (contohnya: 512)\n\n# Tokenisasi data uji dengan padding dan truncation\ntokens = tokenizer(\n    df['text'].tolist(), \n    padding=True, \n    truncation=True, \n    max_length=max_length, \n    return_tensors=\"pt\"\n)\n\n# Konversi label ke tensor\nlabels = torch.tensor(df['Book_name'].astype(int).tolist())\n\n# Membuat DataLoader\ndataset = TensorDataset(tokens['input_ids'], tokens['attention_mask'], labels)\ndataloader = DataLoader(dataset, batch_size=8)  # Menggunakan batch size lebih kecil untuk menghindari masalah memori\n\n# Melakukan prediksi\npredictions = []\nmodel.eval()\nwith torch.no_grad():\n    for batch in dataloader:\n        # Memindahkan tensor ke perangkat yang sesuai (CPU/GPU)\n        input_ids, attention_mask, _ = [t.to(device) for t in batch]\n        outputs = model(input_ids, attention_mask=attention_mask)\n        batch_predictions = torch.argmax(outputs.logits, dim=1)\n        predictions.extend(batch_predictions.cpu().tolist())  # Memindahkan prediksi ke CPU sebelum menyimpannya\n\n# Menampilkan laporan klasifikasi\nreport = classification_report(labels.tolist(), predictions, output_dict=True)\n\n# Mengonversi laporan ke DataFrame untuk visualisasi\nreport_df = pd.DataFrame(report).transpose()\n\n# Menampilkan laporan klasifikasi\nprint(report_df)\n\n# Visualisasi hasil\nplt.figure(figsize=(10, 5))\nsns.heatmap(report_df.iloc[:-1, :-1], annot=True, fmt=\".2f\", cmap='Blues')\nplt.title('Classification Report Heatmap')\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}